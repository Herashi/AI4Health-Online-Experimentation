{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introduction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJekxEDjZ1V7OmmY8pPX58",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdempsey/AI4Health-Online-Experimentation/blob/main/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMH8jI_13OX"
      },
      "source": [
        "# Online learning and experimentation algorithms in mobile health"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvhbY9u0mwXb"
      },
      "source": [
        "## Import necessary \n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qWBt8x2BEW"
      },
      "source": [
        "#Part 1: Overview on Contextual Bandits\n",
        "\n",
        "- For each person in a study, let $t=1,\\ldots, T$ denote a sequence of decision points.  \n",
        "- At each decision time $t$,  we observe a state variable $S_t \\in \\mathbb{R}^p$.  \n",
        "- After observing the state variable $S_t$, the _agent_ decides to take action $A_t \\in \\mathcal{A}$.  \n",
        "- After observing state $S_t$ and taking action $A_t$, the agent receive a reward $R_t$ given by\n",
        "$$\n",
        "R_t = r(S_t, A_t) + \\epsilon_t\n",
        "$$\n",
        "where $r(c,a)$ is a function that maps the state-action pair onto the real line and $\\epsilon_t$ is a random error term, e.g., $\\mathbb{E} [\\epsilon_t] = 0$. \n",
        "- The triple (context, action, reward) at a sequence of decision points defines a _contextual bandit_ setting.  \n",
        "- Here, the goal is to maximize the expected reward at every time point $\\mathbb{E}[R_t \\mid S_t, A_t=a] = r(S_t, a)$. \n",
        "- If we knew the reward function $r: \\mathcal{S} \\times \\mathcal{S} \\to \\mathbb{R}$, then the optimal action given state $s$ is\n",
        "$$\n",
        "a^\\star (s) = \\max_{a \\in \\mathcal{A}} r(s, a)\n",
        "$$\n",
        "\n",
        "### A simple approach:\n",
        "\n",
        "- Consider $\\mathcal{A} =\\{0,1\\}$\n",
        "- Randomize treatment $A_t \\sim \\text{Bern}(p)$ for $t=1,\\ldots,T$\n",
        "- Then for $t>T$, just choose\n",
        "$$\n",
        "\\hat A^\\star_t = \\max_{a \\in A} \\hat r(S_t, a)\n",
        "$$\n",
        "where $\\hat r(s,a)$ is the model fit using the batch data collected.\n",
        "\n",
        "### Linear Contextual Bandit\n",
        "\n",
        "- Assume that the reward structure follows\n",
        "$$\n",
        "r(s,a) = x(s,a)^\\top \\beta \n",
        "$$\n",
        "where $x(s,a) \\in \\mathbb{R}^{p}$ is a $p$-dimensional summary of the state and $\\beta \\in \\mathbb{R}^p$ is an unknown parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIRfY7By1qUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5986b723-7b42-4cee-f863-c9ac0922000e"
      },
      "source": [
        "# Simulation example\n",
        "T = 200 # number of steps\n",
        "\n",
        "## Generate context (normal and binary states)\n",
        "mu, sigma = 0, 1 # mean and standard deviation\n",
        "state1 = np.random.normal(mu, sigma, T) # Continuous state\n",
        "state2 = np.random.binomial(n=1, p = 0.7,size=T) # Binary state\n",
        "state = np.stack((state1,state2), axis = 1) # Compelte State at each time\n",
        "\n",
        "## Generate actions (MRT with probability  )\n",
        "action = np.random.binomial(n=1, p = 0.5,size=T) # Binary state\n",
        "\n",
        "## Generate true reward\n",
        "def reward(state, action):\n",
        "  base_reward = state[0] + 0.3*state[1] \n",
        "  advantage = 0.5*state[0] - 0.7*state[1]\n",
        "  return base_reward + advantage * action\n",
        "\n",
        "y = np.repeat(0.,T)\n",
        "for t in range(T):\n",
        "  y[t] = reward(state[t,:], action[t]) + np.random.normal(0, 1, 1)\n",
        "\n",
        "\n",
        "## Triple\n",
        "triple = np.column_stack((state,action, y))\n",
        "print(\"First 10 entries of state (2D), action, and reward\")\n",
        "print(triple[1:10,:])\n",
        "print(\"\\n\")\n",
        "\n",
        "## Build the design matrix\n",
        "X = state\n",
        "for col in range(2):\n",
        "  temp = np.multiply(state[:,col],action)\n",
        "  X = np.column_stack((X, temp))\n",
        "\n",
        "reg = LinearRegression().fit(X,y)\n",
        "print(\"True coefficients using linear model\")\n",
        "print(np.array([1,0.3,0.5,-0.7]))\n",
        "print(\"Fitted coefficients using linear model\")\n",
        "print(reg.coef_)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 entries of state (2D), action, and reward\n",
            "[[ 0.4889732   0.          0.          0.0970272 ]\n",
            " [-1.65526392  1.          0.         -2.15776888]\n",
            " [ 0.88552154  0.          0.          1.29955221]\n",
            " [-1.21393632  1.          0.          0.97508805]\n",
            " [-1.79251348  1.          0.         -1.62145004]\n",
            " [ 1.42910115  0.          1.          1.68040017]\n",
            " [-2.05180643  1.          0.         -3.29626183]\n",
            " [ 0.2045409   1.          0.          1.70142421]\n",
            " [ 2.92629192  1.          0.          1.87006771]]\n",
            "\n",
            "\n",
            "True coefficients using linear model\n",
            "[ 1.   0.3  0.5 -0.7]\n",
            "Fitted coefficients using linear model\n",
            "[ 1.08134331  0.49970343  0.50450428 -0.64743794]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwolFIRmvIqC"
      },
      "source": [
        "## Question 1: What aspects of the reward impact decision making?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJQPaKw8Ash"
      },
      "source": [
        "- The only thing that impacts decision to choose $a = 1$ or $a=0$ is the _advantage function_:\n",
        "$$\n",
        "A(s) = r(s,1) - r(s,0)\n",
        "$$\n",
        "- In the example above\n",
        "$$\n",
        "A(s) = 0.5 s_{0} - 0.7 s_{1} > 0 \\Rightarrow \\frac{0.5}{0.7} s_0 > s_1\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbnRinwV8DSb"
      },
      "source": [
        "## Question 2: Suppose you observe this sequence and are told you have a `final' decision in state $S_t$.  What decision would you make?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfrZnOLpMKQI"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gejXfi7AvMYD"
      },
      "source": [
        "# Part 1b: Investigating mHealth randomized trial data\n",
        "\n",
        "In HeartSteps V2, decision points are 6 times per day.  We \n",
        "Variables include\n",
        "- XX\n",
        "- XX\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLnC-1K2cJRg"
      },
      "source": [
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2zD3n9JSdCs"
      },
      "source": [
        "## Part 1c: Bandits in mHealth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnmrB37Vv1t"
      },
      "source": [
        "## Part 1d: Going beyond Bandits\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCeGJpT3WhL"
      },
      "source": [
        "# Part 2: Constrained Optimization"
      ]
    }
  ]
}