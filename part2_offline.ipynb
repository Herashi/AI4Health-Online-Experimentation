{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part2_offline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzSEBfeGa+BW6hykVpfLcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdempsey/AI4Health-Online-Experimentation/blob/main/part2_offline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMH8jI_13OX"
      },
      "source": [
        "# Section 2: Synthetic HeartSteps and batch RL\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvhbY9u0mwXb"
      },
      "source": [
        "## Import necessary \n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qWBt8x2BEW"
      },
      "source": [
        "#Part 1: Overview on Contextual Bandits\n",
        "\n",
        "- For each person in a study, let $t=1,\\ldots, T$ denote a sequence of decision points.  \n",
        "- At each decision time $t$,  we observe a state variable $S_t \\in \\mathbb{R}^p$.  \n",
        "- After observing the state variable $S_t$, the _agent_ decides to take action $A_t \\in \\mathcal{A}$.  \n",
        "- After observing state $S_t$ and taking action $A_t$, the agent receive a reward $R_t$ given by\n",
        "$$\n",
        "R_t = r(S_t, A_t) + \\epsilon_t\n",
        "$$\n",
        "where $r(c,a)$ is a function that maps the state-action pair onto the real line and $\\epsilon_t$ is a random error term, e.g., $\\mathbb{E} [\\epsilon_t] = 0$. \n",
        "- The triple (context, action, reward) at a sequence of decision points defines a _contextual bandit_ setting.  \n",
        "- Here, the goal is to maximize the expected reward at every time point $\\mathbb{E}[R_t \\mid S_t, A_t=a] = r(S_t, a)$. \n",
        "- If we knew the reward function $r: \\mathcal{S} \\times \\mathcal{S} \\to \\mathbb{R}$, then the optimal action given state $s$ is\n",
        "$$\n",
        "a^\\star (s) = \\max_{a \\in \\mathcal{A}} r(s, a)\n",
        "$$\n",
        "\n",
        "### A simple approach:\n",
        "\n",
        "- Consider $\\mathcal{A} =\\{0,1\\}$\n",
        "- Randomize treatment $A_t \\sim \\text{Bern}(p)$ for $t=1,\\ldots,T$\n",
        "- Then for $t>T$, just choose\n",
        "$$\n",
        "\\hat A^\\star_t = \\max_{a \\in A} \\hat r(S_t, a)\n",
        "$$\n",
        "where $\\hat r(s,a)$ is the model fit using the batch data collected.\n",
        "- This is exactly equivalent to running an MRT and then using the data to construct an optimal decision rule based on the regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwolFIRmvIqC"
      },
      "source": [
        "## Question 1: What aspects of the reward impact decision making?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJQPaKw8Ash"
      },
      "source": [
        "- The only thing that impacts decision to choose $a = 1$ or $a=0$ is the _advantage function_:\n",
        "$$\n",
        "A(s) = r(s,1) - r(s,0)\n",
        "$$\n",
        "- In the synthetic example from previous section, we have\n",
        "$$\n",
        "A(s) = 0.5 s_{0} - 0.7 s_{1} > 0 \\Rightarrow \\frac{0.5}{0.7} s_0 > s_1\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbnRinwV8DSb"
      },
      "source": [
        "## Question 2: What are the pros of this simple approach?  What are the cons?  \n",
        "\n",
        "- Why may we not want to use a randomized policy to collect data in mobile health?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfrZnOLpMKQI"
      },
      "source": [
        "Pros (non-exhaustive)\n",
        "- Simple algorithm\n",
        "- With sufficient data will construct a 'good' policy\n",
        "- Easy to explain \n",
        "\n",
        "Cons (non-exhaustive)\n",
        "- Exploration is random so we learn slowly about the space\n",
        "- Exploit policy may not be optimal\n",
        "- How do we know that we collected enough data? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gejXfi7AvMYD"
      },
      "source": [
        "# Part 1b: Investigating mHealth randomized trial data\n",
        "\n",
        "- In HeartSteps V2, decision points are 6 times per day.  \n",
        "- An MRT simulator based on Heartsteps V2 has been built in R and is available [here](https://drive.google.com/drive/folders/1rhCWugawTjEnwmagrOPwxNssrgIsnypT?usp=sharing)\n",
        "\n",
        "\n",
        "The __State variable__ includes\n",
        "- __ID__: Numeric id taking values between 1-110\n",
        "- __Day__: Day-in-study (numeric)\n",
        "- __Decision time__: Numeric indicator of decision time per day (1-5)\n",
        "- __Dosage/burden__: Pre-defined function of past pushes (walking + anti-sedentary messages), prior to the current decision time. If there is any message delivered to user's phone (not just intent to treat) between time t and time t+1, e.g., active message at time t and anti-sedentary message between time t and t+1,  the dosage at time t+1 ($X_{t+1}$) is defined as $\\lambda \\cdot X_{t} + 1$. Otherwise, $X_{t+1} = \\lambda * X_{t}$, $\\lambda = 0.95$ set by the analysis of HS V1.\n",
        "- __Engagement Indicator__: Binary indicator of whether the number of screens encountered in app from prior day from 12am to 11:59pm is greater than the 40% quantile of the screens collected.\n",
        "- __Temperature__: Temperature (In Celsius degree) at the current location\n",
        "- __Location__: 1 if at a location other than home or work; 0 if at home or work (pre-specified)\n",
        "- __Variation Indicator__: For each time slot, first calculate the standard deviation of the (possibly imputed) 60-min steps  over the past 7 days.  Let the variation indicator on study day (d+1) to be 1 if the standard deviation calculated at day d is greater or equal to the median of the standard deviations up to day d in the study (excluding the warm up period), where d > 0.\n",
        "- __Pre-treatment Steps__: Log-transformed steps 30 mins prior to the current decision time from the tracker; $\\log(y+0.5)$.\n",
        "- __Square root of steps yesterday__: The square root of step counts from the tracker collected from 12am to 11:59 pm \n",
        "\n",
        "Below we show how to bring the MRT data back into python from the Google drive using [these instructions](https://colab.research.google.com/drive/1cMmtzM7rYc-cpW0fkRiTRb-ySr2UHf1h#scrollTo=XTFHRtl68d40).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oai1YoKBmaX9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnmrB37Vv1t"
      },
      "source": [
        "## Part 1c: Going beyond Bandits (Batch V-Learning)\n",
        "\n",
        "- The above pre-supposed our goal is to construct a policy that maximizes the proximal outcome at each decision time\n",
        "- However, decisions at one time may impact the state at a future time\n",
        "- __Question__: What are some examples of delayed \n",
        "\n",
        "- In this case, we may want to maximize a different objective function.\n",
        "- Here, we consider the maximizing the state-value function,\n",
        "$$\n",
        "V(\\pi, s) = \\sum_{k \\geq 0} \\gamma^k \\mathbb{E}_\\pi \\left[ R_{t+k} \\mid S_t = s \\right]\n",
        "$$\n",
        "where $E_{\\pi}$ denotes the \n",
        "- However, we collected data under an MRT policy, $\\mu$.  So we need to re-express in terms of \n",
        "\\begin{align*}\n",
        "V(\\pi , s) &= \\sum_{k \\geq 0} \\mathbb{E}_\\mu \\left[ \\gamma^k R_{t+k} \\left\\{ \\prod_{v=0}^k \\frac{\\pi(A_{v+t}; S_{v+t})}{\\mu(A_{v+t}; S_{v+t})} \\right\\} \\mid S_t = s \\right]  \\\\\n",
        "&= \\mathbb{E}_\\pi \\left[ \\frac{\\pi(A_{v+t}; S_{v+t})}{\\mu(A_{v+t}; S_{v+t})} \\left( R_t + \\gamma \\sum_{k \\geq 0} \\mathbb{E}_\\pi \\left[ \\gamma^k R_{t+k+1} \\left\\{ \\prod_{v=0}^k \\frac{\\pi(A_{v+t+1}; S_{v+t+1})}{\\mu(A_{v+t+1}; S_{v+t+1})} \\right\\} \\mid S_{t+1} \\right] \\right) \\mid S_t \\right] \\\\\n",
        "\\end{align*}\n",
        "This is an importance-weighted variant of the __Bellman optimality equation__.\n",
        "\n",
        "- Suppose that the state-value function is parametrized according to "
      ]
    }
  ]
}