{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part2_offline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMERFHLa5ImXFJch9vEmuXB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdempsey/AI4Health-Online-Experimentation/blob/main/part2_offline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMH8jI_13OX"
      },
      "source": [
        "# Online learning and experimentation algorithms in mobile health"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvhbY9u0mwXb"
      },
      "source": [
        "## Import necessary \n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qWBt8x2BEW"
      },
      "source": [
        "#Part 1: Overview on Contextual Bandits\n",
        "\n",
        "- For each person in a study, let $t=1,\\ldots, T$ denote a sequence of decision points.  \n",
        "- At each decision time $t$,  we observe a state variable $S_t \\in \\mathbb{R}^p$.  \n",
        "- After observing the state variable $S_t$, the _agent_ decides to take action $A_t \\in \\mathcal{A}$.  \n",
        "- After observing state $S_t$ and taking action $A_t$, the agent receive a reward $R_t$ given by\n",
        "$$\n",
        "R_t = r(S_t, A_t) + \\epsilon_t\n",
        "$$\n",
        "where $r(c,a)$ is a function that maps the state-action pair onto the real line and $\\epsilon_t$ is a random error term, e.g., $\\mathbb{E} [\\epsilon_t] = 0$. \n",
        "- The triple (context, action, reward) at a sequence of decision points defines a _contextual bandit_ setting.  \n",
        "- Here, the goal is to maximize the expected reward at every time point $\\mathbb{E}[R_t \\mid S_t, A_t=a] = r(S_t, a)$. \n",
        "- If we knew the reward function $r: \\mathcal{S} \\times \\mathcal{S} \\to \\mathbb{R}$, then the optimal action given state $s$ is\n",
        "$$\n",
        "a^\\star (s) = \\max_{a \\in \\mathcal{A}} r(s, a)\n",
        "$$\n",
        "\n",
        "### A simple approach:\n",
        "\n",
        "- Consider $\\mathcal{A} =\\{0,1\\}$\n",
        "- Randomize treatment $A_t \\sim \\text{Bern}(p)$ for $t=1,\\ldots,T$\n",
        "- Then for $t>T$, just choose\n",
        "$$\n",
        "\\hat A^\\star_t = \\max_{a \\in A} \\hat r(S_t, a)\n",
        "$$\n",
        "where $\\hat r(s,a)$ is the model fit using the batch data collected.\n",
        "- This is exactly equivalent to running an MRT and then using the data to construct an optimal decision rule based on the regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwolFIRmvIqC"
      },
      "source": [
        "## Question 1: What aspects of the reward impact decision making?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJQPaKw8Ash"
      },
      "source": [
        "- The only thing that impacts decision to choose $a = 1$ or $a=0$ is the _advantage function_:\n",
        "$$\n",
        "A(s) = r(s,1) - r(s,0)\n",
        "$$\n",
        "- In the example above\n",
        "$$\n",
        "A(s) = 0.5 s_{0} - 0.7 s_{1} > 0 \\Rightarrow \\frac{0.5}{0.7} s_0 > s_1\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbnRinwV8DSb"
      },
      "source": [
        "## Question 2: What are the pros of this simple approach?  What are the cons?  \n",
        "\n",
        "- Why may we not want to use a randomized policy to collect data in mobile health?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfrZnOLpMKQI"
      },
      "source": [
        "Pros (non-exhaustive)\n",
        "- Simple algorithm\n",
        "- With sufficient data will construct a 'good' policy\n",
        "- Easy to explain \n",
        "\n",
        "Cons (non-exhaustive)\n",
        "- Exploration is random so we learn slowly about the space\n",
        "- Exploit policy may not be optimal\n",
        "- How do we know that we collected enough data? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gejXfi7AvMYD"
      },
      "source": [
        "# Part 1b: Investigating mHealth randomized trial data\n",
        "\n",
        "- In HeartSteps V2, decision points are 6 times per day.  \n",
        "- An MRT simulator based on Heartsteps V2 has been built in R and is available [here](https://drive.google.com/drive/folders/1rhCWugawTjEnwmagrOPwxNssrgIsnypT?usp=sharing)\n",
        "\n",
        "\n",
        "The __State variable__ includes\n",
        "- __ID__: Numeric id taking values between 1-110\n",
        "- __Day__\n",
        "- __Decision time__ \n",
        "- __Dosage__: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2zD3n9JSdCs"
      },
      "source": [
        "## Part 1c: Bandits in mHealth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnmrB37Vv1t"
      },
      "source": [
        "## Part 1d: Going beyond Bandits\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCeGJpT3WhL"
      },
      "source": [
        "# Part 2: Constrained Optimization"
      ]
    }
  ]
}