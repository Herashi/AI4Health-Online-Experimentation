{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part3_online.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDHjTw8zLX6k2zZLn4WANn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdempsey/AI4Health-Online-Experimentation/blob/main/part3_online.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMH8jI_13OX"
      },
      "source": [
        "# Section 3: Synthetic HeartSteps and personalization\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvhbY9u0mwXb"
      },
      "source": [
        "## Import necessary \n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qWBt8x2BEW"
      },
      "source": [
        "#Part 1: Overview on Contextual Bandits\n",
        "\n",
        "- For each person in a study, let $t=1,\\ldots, T$ denote a sequence of decision points.  \n",
        "- At each decision time $t$,  we observe a state variable $S_t \\in \\mathbb{R}^p$.  \n",
        "- After observing the state variable $S_t$, the _agent_ decides to take action $A_t \\in \\mathcal{A}$.  \n",
        "- After observing state $S_t$ and taking action $A_t$, the agent receive a reward $R_t$ given by\n",
        "$$\n",
        "R_t = r(S_t, A_t) + \\epsilon_t\n",
        "$$\n",
        "where $r(c,a)$ is a function that maps the state-action pair onto the real line and $\\epsilon_t$ is a random error term, e.g., $\\mathbb{E} [\\epsilon_t] = 0$. \n",
        "- The triple (context, action, reward) at a sequence of decision points defines a _contextual bandit_ setting.  \n",
        "- Here, the goal is to maximize the expected reward at every time point $\\mathbb{E}[R_t \\mid S_t, A_t=a] = r(S_t, a)$. \n",
        "- If we knew the reward function $r: \\mathcal{S} \\times \\mathcal{S} \\to \\mathbb{R}$, then the optimal action given state $s$ is\n",
        "$$\n",
        "a^\\star (s) = \\max_{a \\in \\mathcal{A}} r(s, a)\n",
        "$$\n",
        "\n",
        "### Linear Contextual Bandit\n",
        "\n",
        "- Assume that the reward structure follows\n",
        "$$\n",
        "r(s,a) = x(s,a)^\\top \\beta \n",
        "$$\n",
        "where $x(s,a) \\in \\mathbb{R}^{p}$ is a $p$-dimensional summary of the state and $\\beta \\in \\mathbb{R}^p$ is an unknown parameter.\n",
        "- Before, we just wanted to build a good policy after collecting data on some participants.  \n",
        "- Suppose now, we wish to minimize our __regret__ in making sub-optimal decisions on an individual\n",
        "\n",
        "### "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIRfY7By1qUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5986b723-7b42-4cee-f863-c9ac0922000e"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 10 entries of state (2D), action, and reward\n",
            "[[ 0.4889732   0.          0.          0.0970272 ]\n",
            " [-1.65526392  1.          0.         -2.15776888]\n",
            " [ 0.88552154  0.          0.          1.29955221]\n",
            " [-1.21393632  1.          0.          0.97508805]\n",
            " [-1.79251348  1.          0.         -1.62145004]\n",
            " [ 1.42910115  0.          1.          1.68040017]\n",
            " [-2.05180643  1.          0.         -3.29626183]\n",
            " [ 0.2045409   1.          0.          1.70142421]\n",
            " [ 2.92629192  1.          0.          1.87006771]]\n",
            "\n",
            "\n",
            "True coefficients using linear model\n",
            "[ 1.   0.3  0.5 -0.7]\n",
            "Fitted coefficients using linear model\n",
            "[ 1.08134331  0.49970343  0.50450428 -0.64743794]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwolFIRmvIqC"
      },
      "source": [
        "## Question 1: How would you use the warm-start policy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbnRinwV8DSb"
      },
      "source": [
        "## Question 2: What are some potential pitfalls in this approach?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr2rFJpmlXUj"
      },
      "source": [
        "## Exercise:  Code up a simple Thompson sampling algorithm for the next individual in the MRT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLnC-1K2cJRg"
      },
      "source": [
        "## Reading in the data\n",
        "\n",
        "\n",
        "## ADD CODE FOR EXPLORATORY DATA ANALYSIS HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnmrB37Vv1t"
      },
      "source": [
        "## Part 1d: Going beyond Bandits\n",
        "\n",
        "- Bandit algorithms maximize immediate reward\n",
        "- This ignores potential longer term impacts of treatment \n",
        "- __Question__: What are some \n",
        "\n",
        "A __Markov Decision Process__ is \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCeGJpT3WhL"
      },
      "source": [
        "# Part 2: Constrained Optimization"
      ]
    }
  ]
}