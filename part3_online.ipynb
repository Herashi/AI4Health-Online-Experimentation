{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part3_online.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCBx1SahfrIESucPpBpRQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdempsey/AI4Health-Online-Experimentation/blob/main/part3_online.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMH8jI_13OX"
      },
      "source": [
        "# Section 3: Synthetic HeartSteps and personalization\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvhbY9u0mwXb"
      },
      "source": [
        "## Import necessary \n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qWBt8x2BEW"
      },
      "source": [
        "#Part 1: Overview on Contextual Bandits\n",
        "\n",
        "- For each person in a study, let $t=1,\\ldots, T$ denote a sequence of decision points.  \n",
        "- At each decision time $t$,  we observe a state variable $S_t \\in \\mathbb{R}^p$.  \n",
        "- After observing the state variable $S_t$, the _agent_ decides to take action $A_t \\in \\mathcal{A}$.  \n",
        "- After observing state $S_t$ and taking action $A_t$, the agent receive a reward $R_t$ given by\n",
        "$$\n",
        "R_t = r(S_t, A_t) + \\epsilon_t\n",
        "$$\n",
        "where $r(c,a)$ is a function that maps the state-action pair onto the real line and $\\epsilon_t$ is a random error term, e.g., $\\mathbb{E} [\\epsilon_t] = 0$. \n",
        "- The triple (context, action, reward) at a sequence of decision points defines a _contextual bandit_ setting.  \n",
        "- Here, the goal is to maximize the expected reward at every time point $\\mathbb{E}[R_t \\mid S_t, A_t=a] = r(S_t, a)$. \n",
        "- If we knew the reward function $r: \\mathcal{S} \\times \\mathcal{S} \\to \\mathbb{R}$, then the optimal action given state $s$ is\n",
        "$$\n",
        "a^\\star (s) = \\max_{a \\in \\mathcal{A}} r(s, a)\n",
        "$$\n",
        "\n",
        "### Linear Contextual Bandit\n",
        "\n",
        "- Assume that the reward structure follows\n",
        "$$\n",
        "r(s,a) = x(s,a)^\\top \\beta \n",
        "$$\n",
        "where $x(s,a) \\in \\mathbb{R}^{p}$ is a $p$-dimensional summary of the state and $\\beta \\in \\mathbb{R}^p$ is an unknown parameter.\n",
        "- Before, we just wanted to build a good policy after collecting data on some participants.  \n",
        "- Suppose now, we wish to minimize our __expected regret__ in making sub-optimal decisions on an individual\n",
        "$$\n",
        "\\text{Regret}_t = \\sum_{s=1}^t r(S_t, a_t^\\star) - r(S_t, A_t)\n",
        "$$\n",
        "\n",
        "### Recall Susan's Talk!\n",
        "\n",
        "- How does RL attempt to learn a policy that maximizes the reward (minimizes regret)?\n",
        "- Two elements:\n",
        "  - __Learning algorithm__: Used to incrementally update the model\n",
        "  - __Action Selection__: Uses model to select next action\n",
        "- Goal of action selection is to balance between __exploitation__ and __exploration__!\n",
        "\n",
        "### Learning algorithm: Bayesian linear regression\n",
        "\n",
        "- Here, we assume a common prior across participants but independence\n",
        "- That is,\n",
        "$$\n",
        "R_{i, t} = \\phi (S_{i,t}, A_{i,t})^\\top w_i + \\epsilon_{i,t}\n",
        "$$\n",
        "- And each $w_i$ is drawn with independent priors for each individual\n",
        "- We take $\\sigma^2$ to have an inverse-gamma prior and a normal prior for $w$ given $\\sigma^2$ with mean $\\mu_0$ and varaince $\\sigma^2 \\Lambda_0^{-1}$.\n",
        "- __NOTE__: The prior distribution on $w$ can be chosen based on the prior MRT data!\n",
        "- Using the standard Bayesian updates we have the posterior for $\\sigma^2$ is an inverse-gamma with parameters $(a_t, b_t)$ and $w$ given $\\sigma^2$ is normal with parameters $\\mu_t$ and $\\Lambda_t$ satisfying\n",
        "\\begin{align*}\n",
        "\\mu_t &= ( \\phi(S_{1:t},A_{1:t})^\\top \\phi (S_{1:t},A_{1:t}) + \\Lambda_0)^{-1}  (\\Lambda_0 \\mu_0 + \\phi(S_{1:t},A_{1:t})^\\top \\phi (S_{1:t},A_{1:t}) \\hat w) \\\\\n",
        "\\Lambda_t &= (\\phi(S_{1:t},A_{1:t})^\\top \\phi (S_{1:t},A_{1:t}) + \\Lambda_0) \\\\\n",
        "a_t &= a_0 + t/2 \\\\\n",
        "b_t &= b_0 + \\frac{1}{2} (R_{1:t}^\\top R_{1:t} + \\mu_0^\\top \\Lambda_0 \\mu_0 - \\mu_n^\\top \\Lambda_n \\mu_n)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "### Action Selection: Posterior sampling\n",
        "\n",
        "- Sample $\\sigma^2_t \\sim InvGamma(a_t, b_t)$\n",
        "- Sample $w_t \\sim N(\\mu_{t}, \\sigma^2_t \\Lambda_n)$ then\n",
        "$$\n",
        "A_t = \\arg \\max_{a \\in \\mathcal{A}} \\phi(S_t, a)^\\top w_i\n",
        "$$\n",
        "\n",
        "See [here](https://arxiv.org/pdf/2008.01571.pdf) for additional details on a slightly more complicated case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oai1YoKBmaX9"
      },
      "source": [
        "import collections\r\n",
        "\r\n",
        "import glob\r\n",
        "\r\n",
        "# Importing drive method from colab for accessing google drive\r\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxK8XPHH6eNr"
      },
      "source": [
        " ## Importing Dataset from Google Drive\r\n",
        "\r\n",
        "You can just go back and work directly with the MRT simulator based on Heartsteps V2 has been built in R and is available [here](https://drive.google.com/drive/folders/1rhCWugawTjEnwmagrOPwxNssrgIsnypT?usp=sharing)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYgqz-X_4zCX",
        "outputId": "773e5d74-7ad6-412e-c1aa-d35f28d15fc7"
      },
      "source": [
        "# Mounting drive\r\n",
        "# This will require authentication : Follow the steps as guided\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwolFIRmvIqC"
      },
      "source": [
        "## Question 1: How would you use the warm-start policy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbnRinwV8DSb"
      },
      "source": [
        "## Question 2: What are some potential pitfalls in this approach?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr2rFJpmlXUj"
      },
      "source": [
        "## Exercise:  Code up a simple Thompson sampling algorithm for the next individual in the MRT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnmrB37Vv1t"
      },
      "source": [
        "## Part 1b: Online updating the Markov approach\n",
        "\n",
        "- Suppose $\\epsilon$-greedy action selection then update policy as follows:\n",
        "- Suppose that the state-value function is parametrized according to $V(\\pi, s; \\theta^\\pi) = \\Phi (s)^\\prime \\theta$, then define \n",
        "$$\n",
        "\\Lambda_n (\\pi, \\theta^{\\pi}) = \\left[ n^{-1} \\sum_{i=1}^n \\sum_{t=1}^{T_i} \\frac{\\pi(A_{i,t}; S_{i,t})}{\\hat \\pi_n^{v-1}(A_{i,t}; S_{i,t})} \\left( \\gamma \\Phi(S_{i,t}) \\Phi(S_{i,t+1})^\\prime - \\Phi(S_{i,t}) \\Phi(S_{i,t})^\\prime\\right) \\right] \\theta^\\pi + n^{-1}  \\sum_{i=1}^n \\sum_{t=1}^{T_i} \\left[\\frac{\\pi(A_{i,t}; S_{i,t})}{\\hat \\pi_n^{v-1}(A_{i,t}; S_{i,t})} R_{i,t} \\Phi (S_{i,t}) \\right]\n",
        "$$\n",
        "and we can estimate\n",
        "$$\n",
        "\\hat \\theta_n^\\pi = \\arg \\min_{\\theta^\\pi \\in \\Theta} \\left[ \\Lambda_n (\\pi, \\theta^{\\pi})^\\prime \\Lambda_n (\\pi, \\theta^{\\pi}) + \\lambda_n (\\theta^{\\pi})^\\prime \\theta^{\\pi} \\right]\n",
        "$$\n",
        "where $\\lambda_n$ is a tuning parameter.\n",
        "- See [here](https://arxiv.org/pdf/1611.03531v2.pdf) for additional details.\n",
        "\n",
        "- __Question__: We could update every decision point.  What are some pitfalls of updating often?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCeGJpT3WhL"
      },
      "source": [
        "# Additional topics not covered\r\n",
        "\r\n",
        "- Constrained optimization under a randomized constraint\r\n",
        "- Did we achieve personalization?\r\n",
        "- Questions and discussion\r\n"
      ]
    }
  ]
}